{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debamitro/6-s093-day1-finetune-flux/blob/main/6_S093%2C_Day_1_Finetune_Flux_Replicate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use\n",
        "\n",
        "To run and modify the notebook, in the top left go to file -> make a copy in Drive.\n",
        "\n",
        "Useful shortcuts:\n",
        "- Shift + enter: runs a cell\n",
        "\n",
        "Additional Resources:\n",
        "\n",
        "More in depth fine tuning explanation [here](https://civitai.com/articles/4/make-your-own-loras-easy-and-free), [here](https://replicate.com/blog/fine-tune-flux) or [here](https://dreambooth.github.io/)."
      ],
      "metadata": {
        "id": "StqF3MkdMx7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 0.A Install replicate library\n",
        "!pip install replicate"
      ],
      "metadata": {
        "id": "pdPQvzUKAg1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1df675c-08e5-4c1c-82ea-61bacd8ac9ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: replicate in /usr/local/lib/python3.11/dist-packages (1.0.4)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.6)\n",
            "Requirement already satisfied: httpx<1,>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from replicate) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from replicate) (24.2)\n",
            "Requirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.11/dist-packages (from replicate) (2.10.5)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from replicate) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->replicate) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (2.27.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf0Pnm7hELFB"
      },
      "outputs": [],
      "source": [
        "#@title 0.B Setup Replicate\n",
        "\n",
        "import os\n",
        "import replicate\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# YOUR REPLICATE API KEY\n",
        "REPLICATE_API_KEY = \"\"\n",
        "\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 0.C Test the image generation model\n",
        "\n",
        "output = replicate.run(\n",
        "    \"black-forest-labs/flux-dev\",\n",
        "    input={\n",
        "        \"prompt\": \"A photo of DJENNDOG in a space station\",\n",
        "        \"num_inference_steps\": 28, # typically need ~30 for \"dev\" model. Less steps == faster generation, but the quality is worse\n",
        "        \"guidance_scale\": 7.5,     # how much attention the model pays to the prompt. Try different values between 1 and 50 to see\n",
        "        \"model\": \"schnell\",        # after fine-tuning you can use \"schnell\" model to generate images faster. In that case put num_inference_steps=4\n",
        "    }\n",
        ")\n",
        "\n",
        "generated_img_url = str(output[0])\n",
        "print(f\"Generated image URL: {generated_img_url}\")\n",
        "display(Image(url=generated_img_url))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DdOpzTnALChK",
        "outputId": "54bc331a-23c7-4e2f-dd46-cffde720e481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated image URL: https://replicate.delivery/xezq/HZTvMorlSpLmCpgICjfkoZbcrPUWGweBjzycFOCHfDYcUjLoA/out-0.webp\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://replicate.delivery/xezq/HZTvMorlSpLmCpgICjfkoZbcrPUWGweBjzycFOCHfDYcUjLoA/out-0.webp\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Finetuning a text to image model\n",
        "\n",
        "The first and most important thing to care about when training a custom image generation model, is the data. If you have a bad dataset that you are trianing on, it does not matter what model or how much compute you throw at the problem, your output model will still not perform the way that you want it to.\n",
        "\n",
        "For image generation, we dont actually need a lot of data to add a new concept or style to the model, as little as 5 images will do, although more is always better, usually datasets are between 20-1000 miages. When selecting images here's what you need to keep in mind:\n",
        "\n",
        "- Avoid low quality images, i.e. blurry or low (<256 px) resolution\n",
        "- Avoid images with weird aspect ratios (anything more than 2:1, ie 1024x512px)\n",
        "- Dont worry about getting 4k or super high resolution images, they will be downscaled to ~1024px per side when training\n",
        "\n",
        "When training a model, you will typically either be training the model to understand a person, or new style. Because of this, you will usually include a trigger word that lets the model know you are trying to evoke that concept. That way the model will keep its previous understanding of concepts while also having a new one added to it. Because we dont want to overwrite existing concepts, the trigger word will be a specific person's name, or a \"custom\" word, i.e. \"SUNDAI\" or \"tr1gg3r w0rd\"."
      ],
      "metadata": {
        "id": "e-8Z6vXmcUxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.A Create the model repository\n",
        "\n",
        "# Here we are setting up the repository in replicate where the model will go once we have trained it\n",
        "\n",
        "import replicate\n",
        "from replicate.exceptions import ReplicateError\n",
        "\n",
        "# You can see your username on replicate in the top left corner.\n",
        "\n",
        "# NOTE: we use sundai-club account name because you logged via Sundai org\n",
        "replicate_username = \"sundai-club\"\n",
        "# Name of your fintuned model\n",
        "finetuned_mode_name = \"flux-djenydog\"\n",
        "\n",
        "try:\n",
        "  model = replicate.models.create(\n",
        "      owner=replicate_username,\n",
        "      name=finetuned_mode_name,\n",
        "      visibility=\"public\",  # or \"private\" if you prefer\n",
        "      hardware=\"gpu-t4\",  # Replicate will override this for fine-tuned models\n",
        "      description=\"A fine-tuned FLUX.1 model\",\n",
        "  )\n",
        "  print(f\"Model created: {model.name}\")\n",
        "except ReplicateError as e:\n",
        "  if \"already exists\" in e.detail:\n",
        "    print(\"Model already exists, loading it.\")\n",
        "    model = replicate.models.get(f\"{replicate_username}/{finetuned_mode_name}\")\n",
        "  else:\n",
        "    raise e\n",
        "\n",
        "print(f\"Model URL: https://replicate.com/{model.owner}/{model.name}\")"
      ],
      "metadata": {
        "id": "3LukzVaWPRFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5e433d-df7b-4047-cbae-d09d51334ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model already exists, loading it.\n",
            "Model URL: https://replicate.com/sundai-club/flux-djenydog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.B Train the model\n",
        "\n",
        "# The dataset needs to be a zip folder, with public access to it\n",
        "dataset_url = \"https://huggingface.co/datasets/AMead10/Victor-Perkins/resolve/main/vector.zip\"\n",
        "trigger_word = \"DJENNDOG\"\n",
        "steps = 1000\n",
        "\n",
        "training = replicate.trainings.create(\n",
        "    version=\"ostris/flux-dev-lora-trainer:4ffd32160efd92e956d39c5338a9b8fbafca58e03f791f6d8011f3e20e8ea6fa\",\n",
        "    input={\n",
        "        \"input_images\": open(\"djeny_train_data.zip\", \"rb\"),\n",
        "        \"steps\": 1000,\n",
        "    },\n",
        "    trigger_word=trigger_word,\n",
        "    destination=f\"{model.owner}/{model.name}\"\n",
        ")\n",
        "\n",
        "print(f\"Training started: {training.status}\")\n",
        "print(f\"Training URL: https://replicate.com/p/{training.id}\")"
      ],
      "metadata": {
        "id": "ZwloUZKKPSKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e194b96-12f5-427e-e799-5dc569128afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started: starting\n",
            "Training URL: https://replicate.com/p/d2tmsxf1kdrm80cmdgv8v1r8fw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.C Test your fine-tuned model\n",
        "\n",
        "latest_version = model.versions.list()[0]\n",
        "output = replicate.run(\n",
        "    latest_version,\n",
        "    input={\n",
        "        \"prompt\": \"DJENNDOG black dog is now wrapped in a small blanket, pretending to be asleep on the couch, still with visible cookie crumbs around its mouth. A confused-looking human mom is standing in the kitchen doorway, looking at the mess. The dog has one eye slightly open, peeking. Cute cartoonish style, warm colors.\",\n",
        "        \"num_inference_steps\": 28, # typically need ~30 for \"dev\" model. Less steps == faster generation, but the quality is worse\n",
        "        \"guidance_scale\": 7.5,     # how much attention the model pays to the prompt. Try different values between 1 and 50 to see\n",
        "        \"model\": \"dev\",            # after fine-tuning you can use \"schnell\" model to generate images faster. In that case put num_inference_steps=4\n",
        "    }\n",
        ")\n",
        "\n",
        "generated_img_url = str(output[0])\n",
        "print(f\"Generated image URL: {generated_img_url}\")\n",
        "display(Image(url=generated_img_url))"
      ],
      "metadata": {
        "id": "dw8XLZCIx8MO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7c83ff5-2f6f-4f0f-aaf7-6d0fe11772d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated image URL: https://replicate.delivery/xezq/p23v7hOTUYrgIRN5ABwc3IbEbrA4L12cDYeRGArmlgaT14CKA/out-0.webp\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://replicate.delivery/xezq/p23v7hOTUYrgIRN5ABwc3IbEbrA4L12cDYeRGArmlgaT14CKA/out-0.webp\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.D Faster Generation test\n",
        "\n",
        "# TODO: play with the paramaeters of the model to get faster generation of the subject\n",
        "\n",
        "latest_version = model.versions.list()[0]\n",
        "output = replicate.run(\n",
        "    latest_version,\n",
        "    input={\n",
        "        \"prompt\": \"DJENNDOG black dog in a drwing vector style as a favicon\",\n",
        "        \"num_inference_steps\": 8, # typically need ~30 for \"dev\" model. Less steps == faster generation, but the quality is worse\n",
        "        \"guidance_scale\": 7.5,     # how much attention the model pays to the prompt. Try different values between 1 and 50 to see\n",
        "        \"model\": \"schnell\",        # after fine-tuning you can use \"schnell\" model to generate images faster. In that case put num_inference_steps=4\n",
        "    }\n",
        ")\n",
        "\n",
        "generated_img_url = str(output[0])\n",
        "print(f\"Generated image URL: {generated_img_url}\")\n",
        "display(Image(url=generated_img_url))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "6aoxOZuLbOcu",
        "outputId": "25620c73-db85-48c1-8199-61b4774f3e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated image URL: https://replicate.delivery/xezq/zafffNOXVfdSflw0PU5GUS6J1IzAaPXJ80r2qKuG44aBmkugC/out-0.webp\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "2 validation errors for Image\nprompt\n  Field required [type=missing, input_value={'url': 'https://replicat...uG44aBmkugC/out-0.webp'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\ncaptions\n  Field required [type=missing, input_value={'url': 'https://replicat...uG44aBmkugC/out-0.webp'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-e80f57dc3bea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mgenerated_img_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generated image URL: {generated_img_url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerated_img_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for Image\nprompt\n  Field required [type=missing, input_value={'url': 'https://replicat...uG44aBmkugC/out-0.webp'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\ncaptions\n  Field required [type=missing, input_value={'url': 'https://replicat...uG44aBmkugC/out-0.webp'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. (Extra points) Add LLM calls\n",
        "\n",
        "!pip install pydantic openai"
      ],
      "metadata": {
        "id": "vp_7uTZujBXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794dad4d-2144-4db2-a660-ab7c498da629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.10.5)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GITHUB_TOKEN = \"\"\n",
        "\n",
        "import json\n",
        "from pydantic import BaseModel\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://models.inference.ai.azure.com\",\n",
        "    api_key=GITHUB_TOKEN,\n",
        ")"
      ],
      "metadata": {
        "id": "RNFSRut2yGWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: responce_format is better way of enfocing structured output, however it\n",
        "#       is still not supported in GitHub models. If you have an OpenAI key, you\n",
        "#       can use these docs:\n",
        "#         https://platform.openai.com/docs/guides/structured-outputs?lang=python\n",
        "\n",
        "def generate_comics(user_prompt: str):\n",
        "    \"\"\"\n",
        "    Generate comics story with image prompts and captions in JSON format\n",
        "\n",
        "    Args:\n",
        "        user_prompt (str): The story prompt from user\n",
        "\n",
        "    Returns:\n",
        "        dict: JSON formatted comics data\n",
        "    \"\"\"\n",
        "\n",
        "    # System prompt for consistent output formatting\n",
        "    system_prompt = \"\"\"\n",
        "    Create a 3-panel comic story about a dog's adventure. For each panel, provide:\n",
        "    1. An image generation prompt that includes 'DJENNDOG black dog' and ends with 'cartoonish style, warm colors'\n",
        "    2. A caption that refers to the dog as 'Djeny'\n",
        "\n",
        "    Format the output as JSON with this structure:\n",
        "    {\n",
        "        \"comics\": [\n",
        "            {\n",
        "                \"prompt\": \"Image generation prompt here\",\n",
        "                \"caption\": \"Caption text here\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        response_format={ \"type\": \"json_object\" },\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Parse response to JSON\n",
        "    story_json = json.loads(response.choices[0].message.content)\n",
        "    return story_json\n",
        "\n",
        "comics = generate_comics(\"An adventrure in a concert\")\n",
        "print(json.dumps(comics, indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "F3IpEafFbWes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27035f14-b99b-4fa0-8f28-c715842aefe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"comics\": [\n",
            "    {\n",
            "      \"prompt\": \"DJENNDOG black dog wearing a red bandana, excitedly standing at the edge of a forest, colorful music notes floating in the air, trees swaying gently, old comics style, warm colors\",\n",
            "      \"caption\": \"Djeny’s ears perked up as she heard music drifting through the forest—an adventure was calling!\"\n",
            "    },\n",
            "    {\n",
            "      \"prompt\": \"DJENNDOG black dog sneaking through concert grounds, dodging between happy festival-goers and stacks of speakers, food stalls in the background, old comics style, warm colors\",\n",
            "      \"caption\": \"Djeny weaved through the crowd, her nose leading her closer to the tantalizing beats and smells of the concert.\"\n",
            "    },\n",
            "    {\n",
            "      \"prompt\": \"DJENNDOG black dog dancing on stage under colorful lights, musicians laughing and playing instruments behind her, the crowd cheering joyfully, old comics style, warm colors\",\n",
            "      \"caption\": \"To everyone’s delight, Djeny managed to leap on stage, becoming the star of the show with her wagging tail and paw-tapping moves!\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_urls = []\n",
        "for img_description in comics[\"comics\"]:\n",
        "  output = replicate.run(\n",
        "    latest_version,\n",
        "    input={\n",
        "        \"prompt\": img_description[\"prompt\"],\n",
        "        \"num_inference_steps\": 8,  # typically need ~30 for \"dev\" model. Less steps == faster generation, but the quality is worse\n",
        "        \"guidance_scale\": 7.5,     # how much attention the model pays to the prompt. Try different values between 1 and 50 to see\n",
        "        \"model\": \"schnell\",        # after fine-tuning you can use \"schnell\" model to generate images faster. In that case put num_inference_steps=4\n",
        "    }\n",
        "  )\n",
        "\n",
        "  generated_img_url = str(output[0])\n",
        "  img_urls.append(generated_img_url)"
      ],
      "metadata": {
        "id": "9aCQiS463P4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# HTML template for responsive image grid\n",
        "html_template = '''\n",
        "<div style=\"display: flex; flex-wrap: wrap; justify-content: space-around; gap: 20px;\">\n",
        "    {}\n",
        "</div>\n",
        "'''\n",
        "\n",
        "# HTML template for each image and caption\n",
        "image_template = '''\n",
        "<div style=\"flex: 0 1 300px; text-align: center; margin-bottom: 20px;\">\n",
        "    <img src=\"{}\" style=\"max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
        "    <p style=\"margin-top: 10px; font-style: italic; color: #888;\">{}</p>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "# Combine all images and captions\n",
        "image_elements = []\n",
        "for url, caption in zip(img_urls, [ b[\"caption\"] for b in comics[\"comics\"] ]):\n",
        "    image_elements.append(image_template.format(url, caption))\n",
        "\n",
        "# Display the final HTML\n",
        "display(HTML(html_template.format(''.join(image_elements))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "bPO9bQO5wiyd",
        "outputId": "03c0005e-8dac-4ad7-e084-5977bc9a9ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"display: flex; flex-wrap: wrap; justify-content: space-around; gap: 20px;\">\n",
              "    \n",
              "<div style=\"flex: 0 1 300px; text-align: center; margin-bottom: 20px;\">\n",
              "    <img src=\"https://replicate.delivery/xezq/TAt9nOcQq2KYApj44f4KdbcPa3XqT0kJY9rkewQFqO5yF0FUA/out-0.webp\" style=\"max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
              "    <p style=\"margin-top: 10px; font-style: italic; color: #888;\">Djeny’s ears perked up as she heard music drifting through the forest—an adventure was calling!</p>\n",
              "</div>\n",
              "\n",
              "<div style=\"flex: 0 1 300px; text-align: center; margin-bottom: 20px;\">\n",
              "    <img src=\"https://replicate.delivery/xezq/XXaOVxel67VldyvwOwWVGPhAjak24KnD66a5XevfkS6pLoLoA/out-0.webp\" style=\"max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
              "    <p style=\"margin-top: 10px; font-style: italic; color: #888;\">Djeny weaved through the crowd, her nose leading her closer to the tantalizing beats and smells of the concert.</p>\n",
              "</div>\n",
              "\n",
              "<div style=\"flex: 0 1 300px; text-align: center; margin-bottom: 20px;\">\n",
              "    <img src=\"https://replicate.delivery/xezq/MyqRXGYbCtrqMZL31ifyedW6L6kkOpdYl6nqtWf4YAUsLoLoA/out-0.webp\" style=\"max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
              "    <p style=\"margin-top: 10px; font-style: italic; color: #888;\">To everyone’s delight, Djeny managed to leap on stage, becoming the star of the show with her wagging tail and paw-tapping moves!</p>\n",
              "</div>\n",
              "\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6y-tCEgS2eEy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}